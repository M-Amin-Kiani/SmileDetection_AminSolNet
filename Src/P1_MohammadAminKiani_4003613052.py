# -*- coding: utf-8 -*-
"""temp1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_omEumLTFO5MiZY1GM-SJvJxn_tHtGQi

# Smile Classification :)

#### Mohammad Amin Kiani 4003613052
###### Uni of Isfahan - Iran
###### Deep Learning - Dr.Kiani 1404-1403

---------------------------------------------------------------------

## Requirements

#### Genki4K DataSet
"""

# Commented out IPython magic to ensure Python compatibility.
# %ls
from google.colab import files
uploaded = files.upload()

import zipfile
import io

zf = zipfile.ZipFile(io.BytesIO(uploaded['kaggle-genki4k.zip']), "r")
zf.extractall()

"""## All Libs"""

import numpy as np
import os
import cv2
# ------------------------------------------------------------------------
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications import VGG16

from tensorflow.keras import models, layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import dlib
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from keras.optimizers import Adam
from keras.preprocessing import image
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import ReduceLROnPlateau


# from google.colab import drive
# drive.mount('/content/drive')

"""## Face Detect"""

with tf.device('/GPU:0'):

  # مسیر دیتاها
  data_dir = '/content/kaggle-genki4k'
  cropped_dir = '/content/crops'


  # تابع برش چهره‌ها از تصاویر
  # Cascade Classifier برای تشخیص چهره : کتابخانه
  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

  # https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml
  eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')


  # cropped_dir = '/content/crops'
  os.makedirs(cropped_dir, exist_ok=True)

  def crop_face(image_path, save_path, rect=None):
      image = cv2.imread(image_path)
      if image is None:
        print(f"Error loading image: {image_path}")
        return

      # تصویر را به سیاه سفید تبدیل که برای الگوریتم‌های تشخیص چهره است
      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

      if rect is None:
        # اگر RECTANGLE مشخص نشده است، آن را پیدا می‌کنیم
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=3,
            minSize=(30, 30),
            flags=cv2.CASCADE_SCALE_IMAGE
        )

        if len(faces) == 0:

          print(f"No face detected in {image_path}")
          return

        # انتخاب اولین چهره
        rect = faces[0]

      (x, y, w, h) = rect
      face_crop = cv2.resize(image[y:y+h, x:x+w], (224, 224))
      cv2.imwrite(save_path, face_crop)


  # data_dir = '/content/kaggle-genki4k'
  # برش چهره‌ها و ذخیره‌سازی
  for label in ['smile', 'non_smile']:
    label_dir = os.path.join(data_dir, label)
    cropped_label_dir = os.path.join(cropped_dir, label)
    os.makedirs(cropped_label_dir, exist_ok=True)

    for img_name in os.listdir(label_dir):
      img_path = os.path.join(label_dir, img_name)
      save_path = os.path.join(cropped_label_dir, img_name)

      # RECTANGLE برای محدود کردن جستجوی چهره
      image = cv2.imread(img_path)
      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
      faces = face_cascade.detectMultiScale(
          gray,
          scaleFactor=1.012,   # کاهش دادم تا صورت های کوچک تر را هم در بر بگیرد.
          minNeighbors=1,
          minSize=(10, 10),
          flags=cv2.CASCADE_SCALE_IMAGE
      )

      if len(faces) > 0:
        rect = faces[0]  # انتخاب اولین چهره
        crop_face(img_path, save_path, rect)

      else:
        print(f"No face detected in {img_path}")

"""## Download Crops to See"""

# files.download(cropped_dir)

import shutil

# Create a zip file of the crops directory
shutil.make_archive('/content/crops', 'zip', '/content/crops')

files.download('/content/crops.zip')

# Commented out IPython magic to ensure Python compatibility.
# %ls
from google.colab import files
uploaded = files.upload()

import zipfile
import io

zf = zipfile.ZipFile(io.BytesIO(uploaded['fine_crops.zip']), "r")
zf.extractall()

cropped_dir = '/content/fine_crops'

"""## Make & Compile AminNet_Model"""

# # this code helps to read the images faster
# AUTOTUNE = tf.data.AUTOTUNE
# train_generator = train_generator.prefetch(buffer_size=AUTOTUNE)
# validation_generator = validation_generator.prefetch(buffer_size=AUTOTUNE)


# from tensorflow.keras.applications import MobileNetV2
# # ترنسفر لرن بدون هد و فقط بک بون و تعیین اندازه ورودی
# # بارگذاری مدل MobileNet بدون لایه‌های بالایی و تعیین اندازه ورودی
# base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))


IMAGE_SHAPE = [224, 224]
batch_size=10
epochs = 12

base_model = VGG16(input_shape = (224,224,3), weights = 'imagenet', include_top = False)

# from tensorflow.keras.applications import EfficientNetB7
# # Load EfficientNet-B7 model without top layers
# base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze layers  وزن‌ها در حین آموزش تغییر نمی‌کنند. برای جلوگیری از افت کیفیت ویژگی‌های استخراج شده،
# base_model.trainable = False
for layer in base_model.layers:
    layer.trainable = False

x = layers.Flatten()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dense(64, activation='relu')(x)
x = layers.Dense(2, activation='softmax')(x)  # number ofclasses = 2

model = models.Model(inputs=base_model.input, outputs=x)

# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#-----------------------------------------------------------------------------------------------------


# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

# prediction_layer = tf.keras.layers.Dense(1)

# inputs = tf.keras.Input(shape=(224, 224, 3))
# x = preprocess_input(inputs)
# x = base_model(x, training=False)
# x = global_average_layer(x)
# x = tf.keras.layers.Dropout(0.2)(x)
# outputs = prediction_layer(x)
# model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.001
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#               metrics=['accuracy'])

with tf.device('/GPU:0'):

  #   # Prepare dataset
  # train_dataset = image_dataset_from_directory(
  #     cropped_dir,
  #     shuffle=True,
  #     seed=SEED,
  #     image_size=IMAGE_SIZE,
  #     batch_size=BATCH_SIZE,
  #     validation_split=VALIDATION_SPLIT,
  #     subset='training'
  # )

  # validation_dataset = image_dataset_from_directory(
  #     cropped_dir,
  #     shuffle=False,
  #     seed=SEED,
  #     image_size=IMAGE_SIZE,
  #     batch_size=BATCH_SIZE,
  #     validation_split=VALIDATION_SPLIT,
  #     subset='validation'
  # )

  # # Normalize pixel values
  # def normalize_images(image, label):
  #     image = tf.cast(image, tf.float32) / 255.
  #     return image, label

  # train_dataset = train_dataset.map(normalize_images)
  # validation_dataset = validation_dataset.map(normalize_images)

  # # Create data loaders
  # train_loader = tf.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)
  # validation_loader = tf.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, seed=SEED)

  # # Training
  # history = model.fit(
  #     train_loader,
  #     validation_data=validation_loader,
  #     epochs=EPOCHS,
  #     callbacks=[early_stopping, reduce_lr]
  # )

  # print(model.predict(train_loader))

  #----------------------------------------------------------------------------

  # آماده‌سازی داده‌ها با ImageDataGenerator
  # augmentation
  datagen = ImageDataGenerator(
      rescale=1./255,  #  0 255 ==> 0  1
      rotation_range=20,  # تصاویر تا ۲۰ درجه چرخش
      width_shift_range=0.2,  # تصاویر به اندازه ۲۰ درصد از عرض یا ارتفاع خود جابجا
      height_shift_range=0.2,
      shear_range=0.2,  # برش دهی  تا ۲۰ درصد
      zoom_range=0.2,   # زوم بر روی تصاویر تا ۲۰ درصد.
      horizontal_flip=True,  #تصاویر می‌توانند به صورت افقی
      fill_mode='nearest',   # پر کردن پیکسل‌های جدید، نزدیکترین مقدار
      validation_split=0.2)  # 20 درصد داده‌ها برای تست

  train_generator = datagen.flow_from_directory(
      directory=cropped_dir,
      target_size=(224, 224),
      batch_size=10,
      class_mode='categorical',
      subset='training',
      shuffle=True                # تصادفی کردن تصاویر برای جلوگیری از دسترسی‌های بلوکی به داده‌ها
  )

  validation_generator = datagen.flow_from_directory(
      directory=cropped_dir,
      target_size=(224, 224),
      batch_size=10,
      class_mode='categorical',
      subset='validation',
      shuffle=False
  )

#   #------------------------------------------------------------------------

#   # استخراج ویژگی‌ها
#   train_features = model.predict(train_generator)
#   train_labels = train_generator.classes

#   validation_features = model.predict(validation_generator)
#   validation_labels = validation_generator.classes

#   #------------------------------------------------------------------------

  early_stopping = EarlyStopping(monitor='val_loss',   #معیار نظارت بر بهبود مدل.
                                #  min_delta=0.03, # minimium amount of change to count as an improvement
                                patience=2,  # اگر تغییر در معیار بیشتر از ۲ دوره مشاهده نشود، آموزش متوقف می‌شود
                                restore_best_weights=True)  # وزن بهترین مدل ذخیره شده بازگردانده می‌شود.

  reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                                factor=0.2,  #  اگر بهبودی نباشد، نرخ یادگیری به ۲۰ درصد کاهش
                                patience=2,
                                min_lr=1e-6)  # حداقل نرخ

  history = model.fit(
      train_generator,
      validation_data=validation_generator,
      epochs=epochs,
      verbose=1, # نمایش خروجی بیشتر
      # verbose=0: Minimal output. Only shows epoch number and total time.
      #verbose=1: Moderate output. Shows epoch number, validation loss, and accuracy.
      #verbose=2: Detailed output. Shows batch size, iteration number, loss, accuracy, etc.

      callbacks=[early_stopping, reduce_lr]   # استراتژی‌های حضوری برای شرایط خاص
  )

  #برچسب‌های کلاس‌ها و تعداد نمونه‌های موجود در ژنراتور آموزشی
  print(train_generator.classes)
  print(train_generator.samples)

# #-----------------------------------------------------------------------------------------------------

# # model = models.Sequential([
# #     # base_model,
# #     # layers.GlobalAveragePooling2D(),
# #     layers.Flatten(input_shape=train_features.shape[1:]),  # 1D : vector
# #     layers.Dense(128, activation='relu'),
# #     layers.Dropout(0.2),
# #     layers.Dense(1, activation='sigmoid')
# # ])

# #-----------------------------------------------------------------------------------------------------


# # from tensorflow.keras import layers, models
# # from tensorflow.keras.models import Model
# # from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
# # from tensorflow.keras.optimizers import Adam
# # def create_model():
# #     # base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# #     x = base_model.output
# #     x = GlobalAveragePooling2D()(x)
# #     x = Dense(1024, activation='relu')(x)
# #     predictions = Dense(1, activation='sigmoid')(x)

# #     model = Model(inputs=base_model.input, outputs=predictions)

# #     return model

# # model = create_model()

# # # کامپایل مدل
# # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

Y_pred = model.predict(validation_generator, validation_generator.samples / batch_size)

# axix ==> کدام محور (یا ابعاد) آرایه بررسی شود تا بزرگ‌ترین مقدار گرفته شود
# axis=0: بالاترین مقدار در هر ستون (کلاس‌ها)
# axis=1: بالاترین مقدار در هر ردیف (نمونه‌ها)
val_preds = np.argmax(Y_pred, axis=1) #  تعیین کلاس‌های پیش‌بینی شده

import sklearn.metrics as metrics
val_trues =validation_generator.classes  #برچسب‌ها برای مقایسه با پیش‌بینی‌ها

from sklearn.metrics import classification_report
print(classification_report(val_trues, val_preds))  #گزارشی شامل دقت، فراخوانی ، و متوسط دقت

# # آموزش مدل با داده‌های آموزشی و تست روی داده‌های اعتبارسنجی

# history = model.fit(train_features,
#                     train_labels,
#                     batch_size=32,
#                     epochs=initial_epochs,
#                     validation_data=(validation_features, validation_labels),
#                     callbacks=[early_stopping])


# # ارزیابی مدل روی داده‌های اعتبارسنجی
# loss, accuracy = model.evaluate(validation_features, validation_labels)

# print(f"Validation Accuracy: {accuracy * 100:.2f}%")
# print(f"Validation Loss: {loss:.4f}")

# test_loss, test_accuracy = model.evaluate(test_datagen.flow_from_directory(
#     'crops',
#     target_size=(224, 224),
#     class_mode='binary',
#     batch_size=32
# ))

# print(f'Test accuracy: {test_accuracy:.2f}')

# ذخیره مدل
model.save('AminNet_Model_SmileDetector.h5')

files.download('AminNet_Model_SmileDetector.h5')
# قابلیت ذخیره‌سازی داده‌های بزرگ با سرعت بالا
# سازگاری با اکثر کتابخانه‌های عصبی
# امکان ذخیره‌سازی وزن‌ها، ساختار شبکه و متغیرهای حالت

"""# Summary"""

base_model.summary()
model.summary()


import tensorflow as tf
tf.keras.utils.plot_model(model, show_shapes=True)


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('binary_crossentropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# -------------------------------------------------------------------------- tune
base_model.trainable = True  # می‌توانند وزن‌های خود را در طول فرایند آموزش به‌روزرسانی کنند.
fine_tune_at = 100

#(زیر ۱۰۰)، وزن‌ها قابل آموزش نباشند. این به ما کمک می‌کند تا فقط لایه‌های بالایی مدل در طول فرآیند Fine-Tuning آموزش ببینند.
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001/10),
              metrics=['accuracy'])

initial_epochs = 10
fine_tune_epochs = 15
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_generator,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=validation_generator)


acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']
plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""# **Final Result **"""

loss, accuracy = model.evaluate(validation_generator)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print(f"Validation Loss: {loss:.4f}")

model.save('AminNet_Model.h5')

files.download('AminNet_Model.h5')

# from google.colab import drive
# drive.mount('/content')
# # Copy the file to Google Drive
# !cp 'AminNet_Model_SmileDetector.h5' '/content/drive/MyDrive/'

"""# My Video"""

# --------------------------------------- model
from google.colab import files
uploaded = files.upload()

# --------------------------------------- video
from google.colab import files
uploaded = files.upload()

import cv2
import numpy as np
import os
from tensorflow.keras.models import load_model
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input

# Load the trained classifier model
model = load_model('AminNet_Model.h5')

# Load the feature extractor model (VGG16 without the top layers)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# face detector
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# directory to store face crops
if not os.path.exists('Myface_crops'):
    os.makedirs('Myface_crops')

# predict smile or not on a each frame and draw a rectangle
def predict_and_draw(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)   # Convert the image to grayscale for better detection
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    for (x, y, w, h) in faces:
        # Crop and resize face to match input size 224*224
        face_crop = cv2.resize(frame[y:y+h, x:x+w], (224, 224))
        # Expand dimensions to match Keras input requirements
        face_crop_preprocessed = preprocess_input(np.expand_dims(face_crop, axis=0))  # Preprocess the face crop

        # Save the face crop
        cv2.imwrite(f'/content/Myface_crops/face_{x}_{y}.jpg', face_crop)

        # Predict using the classifier model directly on the raw face crop
        prediction = model.predict(face_crop_preprocessed)

        # Draw rectangle based on prediction
        if prediction[0][0] > 0.5:  # treshold for 2 class
            color = (0, 0, 255)  # Red for non-smile
            label = "Non_Smile :( "
        else:
            color = (0, 255, 0)  # Green for smile
            label = "Smile :) "

        # Draw rectangle and label around the face
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2) #2: ضخامت خط مستطیل
        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)  # Display the label متن کمی بالاتر از بالای مستطیل

# process video frame by frame and save the output video
def process_video(video_path, output_path):
    cap = cv2.VideoCapture(video_path)  # Open the video

    # opened successfully?
    if not cap.isOpened():
        print("Error opening video file")
        return

    # Get video properties
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS)) #تعداد فریم‌هایی که در هر ثانیه نمایش داده

    # (Codec) در ویدیو به برنامه یا الگوریتمی اطلاق می‌شود که برای فشرده‌سازی و از بین بردن اطلاعات اضافی ویدیو
    # Define the codec and create VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'mp4v' به معنای استفاده از کدک MP4
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()   # ret : آیا فریم با موفقیت خوانده شده یا خیر

        # ret ==> true   : still has frame

        if not ret:   #  ret ==> False  (یعنی نتوانسته‌ایم فریم را بخوانیم)
            break

        # Predict smile and draw rectangle
        predict_and_draw(frame)

        # Write the frame to the output video
        out.write(frame)

    # Release video capture and writer منابع مربوط به ویدیو را آزاد می‌کنند، تا از نشت حافظه جلوگیری شود.
    cap.release()
    out.release()
    print("Processing completed. SmileDetection Video saved as:", output_path)


# ---------------------------------------------------- predict video by model:

process_video('/content/4003613052_MohammadAminKiani.mp4', '/content/OutPut_SmileDetectionVideo.mp4')

"""Download Pics of me to see each frame & result predict"""

import shutil

# Create a zip file of the crops directory
shutil.make_archive('/content/Myface_crops', 'zip', '/content/Myface_crops')

files.download('/content/Myface_crops.zip')

files.download('/content/OutPut_SmileDetectionVideo.mp4')

#---------------------------------------------------------------------------------
# # Smile Detection with Webcam/Video

# # Define video capture
# cap = cv2.VideoCapture(0) # 0 for webcam, or specify video file path

# # Load the trained model
# model = load_model('AminNet_Model.h5')

# # Function for smile detection on a frame
# def detect_smile(frame):
#   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#   faces = face_cascade.detectMultiScale(gray, 1.1, 4)
#   for (x, y, w, h) in faces:
#     face = frame[y:y + h, x:x + w]
#     face = cv2.resize(face, (224, 224))
#     face_input = preprocess_input(np.expand_dims(face, axis=0))
#     prediction = model.predict(face_input)
#     if prediction[0][0] > 0.5:
#       cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
#       cv2.putText(frame, 'Smile', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
#     else:
#       cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)
#       cv2.putText(frame, 'No Smile', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
#   return frame

# # Process video frames
# while True:
#   ret, frame = cap.read()
#   if not ret:
#     break
#   frame = detect_smile(frame)
#   cv2.imshow('Smile Detection', frame)
#   if cv2.waitKey(1) & 0xFF == ord('q'):
#     break

# # Release resources
# cap.release()
# cv2.destroyAllWindows()